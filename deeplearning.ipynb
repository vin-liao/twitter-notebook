{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L3sbjocVhn4P",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras\n",
        "!pip install -U -q PyDrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GlobalMaxPooling1D, CuDNNGRU, Dropout, BatchNormalization, Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import keras.optimizers\n",
        "import os\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5HXZVblNlknd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#change this value depending on what you need\n",
        "use_existing_weights = True\n",
        "filepath = 'weights_best.hdf5'\n",
        "model_name = 'rnn_model_twitter.h5'\n",
        "\n",
        "if use_existing_weights:\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  # This only needs to be done once per notebook.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  #download weights from google drive\n",
        "  weight_id = '1cM_aXUHcVylDRu9n57L-6Q0L8XmdCvJU'\n",
        "  weight_drive_file = drive.CreateFile({'id': weight_id})\n",
        "  weight_drive_file.GetContentFile(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YBoN65yiZ6yO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip && unzip trainingandtestdata.zip -d data/\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip && unzip glove.twitter.27B.zip -d data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQY_M0oec_Zd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#load data to the working environment\n",
        "data_path = os.path.join(os.path.expanduser('~'), 'data', 'training.1600000.processed.noemoticon.csv')\n",
        "dataset = pd.read_csv(data_path, error_bad_lines=False, encoding='latin1', header=None, names=['sentiment', 'id', 'date', 'flag', 'user', 'text'])\n",
        "dataset.dropna(axis=0, inplace=True)\n",
        "embedding_path = os.path.join(os.path.expanduser('~'), 'data', 'glove.twitter.27B.200d.txt')\n",
        "dim_size = 200\n",
        "# dataset = dataset.sample(frac=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odC1vmZ-kXRe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#preprocess dataset\n",
        "eyes = r\"[8:=;]\"\n",
        "nose = r\"['`\\-]?\"\n",
        "\n",
        "dataset['text'] = dataset['text']\\\n",
        ".str.replace(r'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*', '<url>')\\\n",
        "  .str.replace(r'@\\w+', '<user>')\\\n",
        "  .str.replace(r'{}{}[)dD]+|[)dD]+{}{}'.format(eyes, nose, nose, eyes), '<smile>')\\\n",
        "  .str.replace(r'{}{}p+'.format(eyes, nose), '<lolface>')\\\n",
        "  .str.replace(r'{}{}\\(+|\\)+{}{}'.format(eyes, nose, nose, eyes), '<sadface>')\\\n",
        "  .str.replace(r'{}{}[\\/|l*]'.format(eyes, nose), '<neutralface>')\\\n",
        "  .str.replace(r'/',' / ')\\\n",
        "  .str.replace(r'<3','<heart>')\\\n",
        "  .str.replace(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', '<number>')\\\n",
        "  .str.replace(r'#\\S+', '<hashtag>')\\\n",
        "  .str.replace(r'([!?.]){2,}', r'\\1 <repeat>')\\\n",
        "  .str.replace(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2 <elong>')\n",
        "  \n",
        "#source: https://gist.github.com/tokestermw/cb87a97113da12acb388"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2IfnbVxkutx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#prep the dataset\n",
        "\n",
        "#replacing positive labels from 4 to 1\n",
        "dataset['sentiment'] = dataset['sentiment'].replace(4, 1)\n",
        "\n",
        "#filter these things from the text\n",
        "token = text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\]^_`{|}~\\t\\n')\n",
        "max_len = dataset['text'].str.len().max()\n",
        "\n",
        "#learn the vocabulary from all the text\n",
        "token.fit_on_texts(list(dataset['text']))\n",
        "vocab_size = len(token.word_index) + 1\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataset['text'],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tdataset['sentiment'],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.2)\n",
        "\n",
        "#encode\n",
        "x_train_enc = token.texts_to_sequences(x_train)\n",
        "x_test_enc = token.texts_to_sequences(x_test)\n",
        "\n",
        "#add zero padding\n",
        "x_train_enc_pad = sequence.pad_sequences(x_train_enc, maxlen=max_len)\n",
        "x_test_enc_pad = sequence.pad_sequences(x_test_enc, maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiYcAzlqk-vB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#create embedding dictionary\n",
        "embeddings_index = dict()\n",
        "f = open(embedding_path)\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "#map the vocabulary to it's word embedding\n",
        "embedding_matrix = np.zeros((vocab_size, dim_size))\n",
        "for word, i in token.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rlaZzWaSlQpW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#deep learning\n",
        "finished_training = False\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, dim_size, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(CuDNNGRU(32, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(CuDNNGRU(16, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "if use_existing_weights:\n",
        "  model.load_weights(filepath)\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "earlystop = EarlyStopping(monitor='acc', patience=10, verbose=1, mode='auto')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='auto')\n",
        "callback_lists = [earlystop, checkpoint]\n",
        "\n",
        "model.fit(x_train_enc_pad, y_train, epochs=100, batch_size=3000, callbacks=callback_lists)\n",
        "scores = model.evaluate(x_test_enc_pad, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "model.save(model_name)\n",
        "finished_training = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Bim5mKUxuti",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create & upload a text file.\n",
        "uploaded = drive.CreateFile({'title': filepath})\n",
        "uploaded.SetContentFile(filepath)\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "print('Done uploading weights')\n",
        "\n",
        "if finished_training:\n",
        "  uploaded_model = drive.CreateFile({'title': model_name})\n",
        "  uploaded_model.SetContentFile(model_name)\n",
        "  uploaded_model.Upload()\n",
        "  print('Uploaded file with ID {}'.format(uploaded_model.get('id')))\n",
        "  print('Done uploading model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8JO49khluSvq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#using the model\n",
        "\n",
        "done = False\n",
        "while done == False:\n",
        "  sentence = input('Input sentence: ')\n",
        "  \n",
        "  sentence_encoding = token.texts_to_sequences([sentence])\n",
        "  padded_sentence = sequence.pad_sequences(sentence_encoding, maxlen=max_len)\n",
        "  \n",
        "  prediction = model.predict(np.array(padded_sentence))\n",
        "  if prediction[0] == 1:\n",
        "    print(prediction[0])\n",
        "    print('Positive')\n",
        "  elif prediction[0] == 0:\n",
        "    print(prediction[0])\n",
        "    print('Negative')\n",
        "  \n",
        "  finish = input('Do you still want to input another text? [y/n]')\n",
        "  if finish == 'N' or finish == 'n':\n",
        "    done = True\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}