{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L3sbjocVhn4P",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8eaf1eef-11a7-46ce-b3c4-ec7130419d76",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520044685670,
          "user_tz": -420,
          "elapsed": 9186,
          "user": {
            "displayName": "Vincent",
            "photoUrl": "//lh3.googleusercontent.com/-LlNb3300Jzo/AAAAAAAAAAI/AAAAAAAAAFI/S7-qu260Jj8/s50-c-k-no/photo.jpg",
            "userId": "104593263505885814568"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras\n",
        "!pip install -U -q PyDrive\n",
        "!pip install -q h5py\n",
        "!pip install -q hyperas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GlobalMaxPooling1D, CuDNNGRU, Dropout, BatchNormalization, Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import keras.optimizers\n",
        "import os\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from html import unescape"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5HXZVblNlknd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3f5c00b-387c-4a16-b9e3-16950a8f9c31",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520044693289,
          "user_tz": -420,
          "elapsed": 7579,
          "user": {
            "displayName": "Vincent",
            "photoUrl": "//lh3.googleusercontent.com/-LlNb3300Jzo/AAAAAAAAAAI/AAAAAAAAAFI/S7-qu260Jj8/s50-c-k-no/photo.jpg",
            "userId": "104593263505885814568"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "filepath = 'weights_best.hdf5'\n",
        "model_name = 'rnn_model_twitter.h5'\n",
        "\n",
        "def authenticate_drive():\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  \n",
        "  return drive\n",
        "\n",
        "dl_weight = input(\"Do you want to download the weights from google drive? \")\n",
        "if dl_weight == 'y' or dl_weight == 'Y':\n",
        "  drive = authenticate_drive()\n",
        "    \n",
        "  #download weights from google drive\n",
        "  weight_id = '15N9WaKK5PBet_0MX7EnyxQ8UuXOPuTSW'\n",
        "  weight_drive_file = drive.CreateFile({'id': weight_id})\n",
        "  weight_drive_file.GetContentFile(filepath)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do you want to download the weights from google drive? y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YBoN65yiZ6yO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip && unzip glove.twitter.27B.zip -d data/\n",
        "!wget http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip && unzip Sentiment-Analysis-Dataset.zip -d data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQY_M0oec_Zd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#load data to the working environment\n",
        "data_path = os.path.join(os.path.expanduser('~'), 'data', 'Sentiment Analysis Dataset.csv')\n",
        "dataset = pd.read_csv(data_path, error_bad_lines=False, encoding='utf-8')\n",
        "dataset.dropna(axis=0, inplace=True)\n",
        "dataset = dataset.rename(index=str, columns={\"SentimentText\": \"text\", \"Sentiment\": \"sentiment\"})\n",
        "\n",
        "embedding_path = os.path.join(os.path.expanduser('~'), 'data', 'glove.twitter.27B.100d.txt')\n",
        "dim_size = 100\n",
        "\n",
        "#take sample of it\n",
        "dataset = dataset.sample(frac=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odC1vmZ-kXRe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#preprocess dataset\n",
        "eyes = r\"[8:=;]\"\n",
        "nose = r\"['`\\-]?\"\n",
        "\n",
        "#decode html entities\n",
        "dataset.text = dataset.text.apply(lambda x: unescape(x))\n",
        "\n",
        "#fix this\n",
        "# ã?Ÿã?„ã?“ã‚Œã‚“ã?—ã‚…ã?† at index 1502\n",
        "\n",
        "dataset['text'] = dataset['text']\\\n",
        ".str.replace(r'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*', '<url>')\\\n",
        "  .str.replace(r'@\\w+', '<user>')\\\n",
        "  .str.replace(r'{}{}[)dD]+|[)dD]+{}{}'.format(eyes, nose, nose, eyes), '<smile>')\\\n",
        "  .str.replace(r'{}{}p+'.format(eyes, nose), '<lolface>')\\\n",
        "  .str.replace(r'{}{}\\(+|\\)+{}{}'.format(eyes, nose, nose, eyes), '<sadface>')\\\n",
        "  .str.replace(r'{}{}[\\/|l*]'.format(eyes, nose), '<neutralface>')\\\n",
        "  .str.replace(r'/',' / ')\\\n",
        "  .str.replace(r'<3','<heart>')\\\n",
        "  .str.replace(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', '<number>')\\\n",
        "  .str.replace(r'#\\S+', '<hashtag>')\\\n",
        "  .str.replace(r'([!?.]){2,}', r'\\1 <repeat>')\\\n",
        "  .str.replace(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2 <elong>')\n",
        "  \n",
        "#source: https://gist.github.com/tokestermw/cb87a97113da12acb388"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2IfnbVxkutx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#prep the dataset\n",
        "#filter these things from the text\n",
        "token = text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\]^_`{|}~\\t\\n')\n",
        "max_len = dataset['text'].str.len().max()\n",
        "\n",
        "#learn the vocabulary from all the text\n",
        "token.fit_on_texts(list(dataset['text']))\n",
        "vocab_size = len(token.word_index) + 1\n",
        "\n",
        "#this might produce some error, test these 2 lines\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataset['text'], dataset['sentiment'], test_size=0.015, shuffle=False, random_state=42)\n",
        "\n",
        "y_train = pd.get_dummies(y_train)\n",
        "y_test = pd.get_dummies(y_test)\n",
        "\n",
        "#encode\n",
        "x_train_enc = token.texts_to_sequences(x_train)\n",
        "x_test_enc = token.texts_to_sequences(x_test)\n",
        "\n",
        "#add zero padding\n",
        "x_train_enc_pad = sequence.pad_sequences(x_train_enc, maxlen=max_len)\n",
        "x_test_enc_pad = sequence.pad_sequences(x_test_enc, maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiYcAzlqk-vB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#create embedding dictionary\n",
        "embeddings_index = dict()\n",
        "f = open(embedding_path)\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "#map the vocabulary to it's word embedding\n",
        "embedding_matrix = np.zeros((vocab_size, dim_size))\n",
        "for word, i in token.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArUMjvYwFSj0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "dd96f2cc-e46e-4f3d-a8c1-914892ce2b4a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520044769334,
          "user_tz": -420,
          "elapsed": 750,
          "user": {
            "displayName": "Vincent",
            "photoUrl": "//lh3.googleusercontent.com/-LlNb3300Jzo/AAAAAAAAAAI/AAAAAAAAAFI/S7-qu260Jj8/s50-c-k-no/photo.jpg",
            "userId": "104593263505885814568"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#checking if data works correctly\n",
        "index = random.randint(1, x_train.shape[0])\n",
        "\n",
        "print('Preprocessed sentence')\n",
        "print(dataset.iloc[index, 3], end='\\n\\n')\n",
        "print('Encoded text')\n",
        "print(x_train_enc[index], end='\\n\\n')\n",
        "\n",
        "\n",
        "res = dict((v,k) for k,v in token.word_index.items())\n",
        "for num in x_train_enc[index]:\n",
        "  print(res.get(num), end=' ')\n",
        "\n",
        "print()\n",
        "print('Zero padding')\n",
        "print(x_train_enc_pad[index])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessed sentence\n",
            "<user> Those kebabs were rather good. Thanks heaps \n",
            "\n",
            "Encoded text\n",
            "[1, 284, 28479, 160, 773, 33, 85, 3599]\n",
            "\n",
            "<user> those kebabs were rather good thanks heaps \n",
            "Zero padding\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     1   284 28479   160   773    33    85  3599]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rlaZzWaSlQpW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 153
            },
            {
              "item_id": 300
            },
            {
              "item_id": 497
            },
            {
              "item_id": 717
            },
            {
              "item_id": 951
            },
            {
              "item_id": 1161
            },
            {
              "item_id": 1371
            },
            {
              "item_id": 1590
            },
            {
              "item_id": 1787
            },
            {
              "item_id": 1979
            },
            {
              "item_id": 2215
            },
            {
              "item_id": 2454
            },
            {
              "item_id": 2688
            },
            {
              "item_id": 2911
            },
            {
              "item_id": 3102
            },
            {
              "item_id": 3326
            },
            {
              "item_id": 3530
            },
            {
              "item_id": 3739
            },
            {
              "item_id": 3948
            },
            {
              "item_id": 4169
            },
            {
              "item_id": 4389
            },
            {
              "item_id": 4578
            },
            {
              "item_id": 4785
            },
            {
              "item_id": 5011
            },
            {
              "item_id": 5236
            },
            {
              "item_id": 5458
            },
            {
              "item_id": 5682
            },
            {
              "item_id": 5905
            },
            {
              "item_id": 6091
            },
            {
              "item_id": 6301
            },
            {
              "item_id": 6465
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 2179
        },
        "outputId": "81e9cbd6-d527-413b-8b2d-b47327c6b701",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520049235027,
          "user_tz": -420,
          "elapsed": 2890990,
          "user": {
            "displayName": "Vincent",
            "photoUrl": "//lh3.googleusercontent.com/-LlNb3300Jzo/AAAAAAAAAAI/AAAAAAAAAFI/S7-qu260Jj8/s50-c-k-no/photo.jpg",
            "userId": "104593263505885814568"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#deep learning\n",
        "finished_training = False\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, dim_size, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "\n",
        "model.add(CuDNNGRU(64, return_sequences=True))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "#gru 64 dense 32 underfit loss: 0.3738 - val_loss: 0.4187\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "use_existing_weights = True\n",
        "if use_existing_weights:\n",
        "  print('Training network using existing weights')\n",
        "  model.load_weights(filepath)\n",
        "else:\n",
        "  print('Training network from scratch')\n",
        "\n",
        "#hyperparameter is a mess, tune it\n",
        "#lr, batch_size, epoch, dropout, maybe some decay, hidden units is maybe too small\n",
        "opt = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "#am i even using mini batch right now, i'm not sure lmao\n",
        "\n",
        "#grid search here, try lr value, dropout, layer size, do it for 20-30 epoch\n",
        "#glove dimension also affects overfitting by the way\n",
        "earlystop = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='auto')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callback_lists = [earlystop, checkpoint]\n",
        "\n",
        "model.fit(x_train_enc_pad, y_train, epochs=300, batch_size=512, callbacks=callback_lists, verbose=1, validation_split=0.015)\n",
        "scores = model.evaluate(x_test_enc_pad, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "model.save(model_name)\n",
        "finished_training = True"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training network using existing weights\n",
            "Train on 382902 samples, validate on 5831 samples\n",
            "Epoch 1/300\n",
            "270848/382902 [====================>.........] - ETA: 28s - loss: 0.3971 - acc: 0.8206"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3971 - acc: 0.8205 - val_loss: 0.4218 - val_acc: 0.8062\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.42178, saving model to weights_best.hdf5\n",
            "Epoch 2/300\n",
            "130560/382902 [=========>....................] - ETA: 1:02 - loss: 0.3906 - acc: 0.8237"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 250us/step - loss: 0.3946 - acc: 0.8221 - val_loss: 0.4109 - val_acc: 0.8146\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.42178 to 0.41091, saving model to weights_best.hdf5\n",
            "Epoch 3/300\n",
            " 91648/382902 [======>.......................] - ETA: 1:13 - loss: 0.3904 - acc: 0.8249"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3927 - acc: 0.8233 - val_loss: 0.4300 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00003: val_loss did not improve\n",
            "Epoch 4/300\n",
            " 96768/382902 [======>.......................] - ETA: 1:11 - loss: 0.3905 - acc: 0.8238"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3909 - acc: 0.8241 - val_loss: 0.4148 - val_acc: 0.8128\n",
            "\n",
            "Epoch 00004: val_loss did not improve\n",
            "Epoch 5/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3868 - acc: 0.8271"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 97s 253us/step - loss: 0.3890 - acc: 0.8259 - val_loss: 0.4087 - val_acc: 0.8138\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.41091 to 0.40872, saving model to weights_best.hdf5\n",
            "Epoch 6/300\n",
            " 82944/382902 [=====>........................] - ETA: 1:15 - loss: 0.3843 - acc: 0.8268"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3872 - acc: 0.8264 - val_loss: 0.4271 - val_acc: 0.8016\n",
            "\n",
            "Epoch 00006: val_loss did not improve\n",
            "Epoch 7/300\n",
            " 94208/382902 [======>.......................] - ETA: 1:12 - loss: 0.3827 - acc: 0.8286"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3857 - acc: 0.8272 - val_loss: 0.4324 - val_acc: 0.7980\n",
            "\n",
            "Epoch 00007: val_loss did not improve\n",
            "Epoch 8/300\n",
            " 97792/382902 [======>.......................] - ETA: 1:11 - loss: 0.3839 - acc: 0.8283"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3844 - acc: 0.8276 - val_loss: 0.4103 - val_acc: 0.8165\n",
            "\n",
            "Epoch 00008: val_loss did not improve\n",
            "Epoch 9/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3829 - acc: 0.8288"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3833 - acc: 0.8285 - val_loss: 0.4231 - val_acc: 0.8134\n",
            "\n",
            "Epoch 00009: val_loss did not improve\n",
            "Epoch 10/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3805 - acc: 0.8306"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3814 - acc: 0.8294 - val_loss: 0.4188 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00010: val_loss did not improve\n",
            "Epoch 11/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3768 - acc: 0.8314"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3804 - acc: 0.8298 - val_loss: 0.4192 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00011: val_loss did not improve\n",
            "Epoch 12/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3763 - acc: 0.8325"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3787 - acc: 0.8314 - val_loss: 0.4111 - val_acc: 0.8155\n",
            "\n",
            "Epoch 00012: val_loss did not improve\n",
            "Epoch 13/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3768 - acc: 0.8326"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3780 - acc: 0.8321 - val_loss: 0.4101 - val_acc: 0.8132\n",
            "\n",
            "Epoch 00013: val_loss did not improve\n",
            "Epoch 14/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3748 - acc: 0.8334"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 97s 252us/step - loss: 0.3761 - acc: 0.8322 - val_loss: 0.4170 - val_acc: 0.8096\n",
            "\n",
            "Epoch 00014: val_loss did not improve\n",
            "Epoch 15/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3734 - acc: 0.8339"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3750 - acc: 0.8330 - val_loss: 0.4176 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00015: val_loss did not improve\n",
            "Epoch 16/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3689 - acc: 0.8361"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3744 - acc: 0.8330 - val_loss: 0.4271 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00016: val_loss did not improve\n",
            "Epoch 17/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3700 - acc: 0.8340"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3729 - acc: 0.8336 - val_loss: 0.4299 - val_acc: 0.7992\n",
            "\n",
            "Epoch 00017: val_loss did not improve\n",
            "Epoch 18/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3705 - acc: 0.8355"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 97s 252us/step - loss: 0.3715 - acc: 0.8348 - val_loss: 0.4117 - val_acc: 0.8129\n",
            "\n",
            "Epoch 00018: val_loss did not improve\n",
            "Epoch 19/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3689 - acc: 0.8365"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3703 - acc: 0.8353 - val_loss: 0.4143 - val_acc: 0.8126\n",
            "\n",
            "Epoch 00019: val_loss did not improve\n",
            "Epoch 20/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3656 - acc: 0.8381"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 250us/step - loss: 0.3696 - acc: 0.8359 - val_loss: 0.4273 - val_acc: 0.7992\n",
            "\n",
            "Epoch 00020: val_loss did not improve\n",
            "Epoch 21/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3648 - acc: 0.8383"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 250us/step - loss: 0.3682 - acc: 0.8364 - val_loss: 0.4222 - val_acc: 0.8035\n",
            "\n",
            "Epoch 00021: val_loss did not improve\n",
            "Epoch 22/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3632 - acc: 0.8386"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3677 - acc: 0.8366 - val_loss: 0.4173 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00022: val_loss did not improve\n",
            "Epoch 23/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3645 - acc: 0.8389"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 250us/step - loss: 0.3663 - acc: 0.8376 - val_loss: 0.4312 - val_acc: 0.8005\n",
            "\n",
            "Epoch 00023: val_loss did not improve\n",
            "Epoch 24/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3593 - acc: 0.8401"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 250us/step - loss: 0.3656 - acc: 0.8381 - val_loss: 0.4196 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00024: val_loss did not improve\n",
            "Epoch 25/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3623 - acc: 0.8397"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 250us/step - loss: 0.3639 - acc: 0.8387 - val_loss: 0.4158 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00025: val_loss did not improve\n",
            "Epoch 26/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3574 - acc: 0.8420"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 252us/step - loss: 0.3633 - acc: 0.8390 - val_loss: 0.4241 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00026: val_loss did not improve\n",
            "Epoch 27/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:11 - loss: 0.3565 - acc: 0.8423"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3626 - acc: 0.8397 - val_loss: 0.4180 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00027: val_loss did not improve\n",
            "Epoch 28/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3572 - acc: 0.8427"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3621 - acc: 0.8400 - val_loss: 0.4165 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00028: val_loss did not improve\n",
            "Epoch 29/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3580 - acc: 0.8424"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 251us/step - loss: 0.3612 - acc: 0.8405 - val_loss: 0.4137 - val_acc: 0.8154\n",
            "\n",
            "Epoch 00029: val_loss did not improve\n",
            "Epoch 30/300\n",
            " 98816/382902 [======>.......................] - ETA: 1:10 - loss: 0.3523 - acc: 0.8453"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "382902/382902 [==============================] - 96s 250us/step - loss: 0.3605 - acc: 0.8408 - val_loss: 0.4265 - val_acc: 0.8044\n",
            "\n",
            "Epoch 00030: val_loss did not improve\n",
            "Epoch 00030: early stopping\n",
            "5920/5920 [==============================] - 3s 569us/step\n",
            "\n",
            "acc: 79.26%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Bim5mKUxuti",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "weight_choice = input('Do you want to upload the weight into google drive? ')\n",
        "if weight_choice == 'y' or weight_choice == 'Y':\n",
        "  drive = authenticate_drive()\n",
        "\n",
        "  #upload process error\n",
        "  weight_upload = drive.CreateFile({'title': filepath, 'id': '15N9WaKK5PBet_0MX7EnyxQ8UuXOPuTSW'})\n",
        "  weight_upload.SetContentFile(filepath)\n",
        "  weight_upload.Upload()\n",
        "  print('Uploaded file with ID {}'.format(weight_upload.get('id')))\n",
        "  print('Done uploading weights')\n",
        "\n",
        "model_choice = input('Do you want to upload the model into google drive? ')\n",
        "if (model_choice == 'y' or model_choice == 'Y') and finished_training:\n",
        "  model_upload = drive.CreateFile({'title': model_name})\n",
        "  model_upload.SetContentFile(model_name)\n",
        "  model_upload.Upload()\n",
        "  print('Uploaded file with ID {}'.format(model_upload.get('id')))\n",
        "  print('Done uploading model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8JO49khluSvq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#using the model\n",
        "\n",
        "done = False\n",
        "while done == False:\n",
        "  sentence = input('Input sentence: ')\n",
        "  \n",
        "  sentence_encoding = token.texts_to_sequences([sentence])\n",
        "  padded_sentence = sequence.pad_sequences(sentence_encoding, maxlen=max_len)\n",
        "  \n",
        "  prediction = model.predict(np.array(padded_sentence))\n",
        "  if prediction[0] == 1:\n",
        "    print(prediction[0])\n",
        "    print('Positive')\n",
        "  elif prediction[0] == 0:\n",
        "    print(prediction[0])\n",
        "    print('Negative')\n",
        "  \n",
        "  finish = input('Do you still want to input another text? [y/n]')\n",
        "  if finish == 'N' or finish == 'n':\n",
        "    done = True"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}